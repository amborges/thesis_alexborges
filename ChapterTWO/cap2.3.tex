\subsection{Aprendizado de Máquina Supervisionado}
\label{cap:2.3}

Como foi possível observar no início do capítulo \ref{cap:2}, a execução de um codificador de vídeo requer um elevado custo computacional, principalmente quando se utilizam codificadores baseados em formatos ou padrões mais atuais. Em parte, isso se relaciona com o aumento da complexidade das técnicas implementadas para realizar as predições e as compressões de dados. Uma das formas implementadas para reduzir o custo computacional é por meio do uso de algoritmos de aprendizado de máquina para acelerar tomadas de decisão do codificador. Dessa maneira, também se espera que o mesmo aconteça quando se utilizam transcodificadores de vídeo em que o conjunto de dados presentes durante o fluxo de execução do transcodificador pode ser reaproveitado pelos algoritmos de aprendizado de máquina. Como ficou mais claro na seção \ref{cap:2.2}, na área de codificação de vídeo, usam-se principalmente (mas não exclusivamente) os algoritmos de aprendizado supervisionado, já que é possível estruturar o conjunto de dados de treinamento em concordância com o rótulo de saída esperado.

Importante ressaltar que os rótulos de saída já são conhecidos, pois é possível executar o processo de codificação ou de transcodificação original de modo a se obter os valores durante e após a sua execução, o que justifica a maior presença dessa categoria de aprendizado de máquina nos trabalhos existentes na literatura científica. Como será explorado com mais detalhes no capítulo \ref{cap:3}, os algoritmos de aprendizado de máquina mais utilizados nos trabalhos de transcodificação de vídeo se baseiam em um dos três principais conceitos: árvores de decisão, classificadores lineares ou aprendizagem profunda.

O conceito de aprendizado de máquina baseado em árvore de decisão engloba algoritmos que geram uma estrutura de sequência de condicionais que parte do teste principal (com o atributo de maior relevância) até as respostas finais (folhas da árvore). Em razão disso, é facilmente implementável, tanto em software como em hardware, pois o treinamento e a execução do modelo não apresentam dificuldades. Podemos encontrar alguns algoritmos baseados em árvore de decisão, sendo que os mais comuns são o C4.5 \cite{bib:quinlan_2014}, o C5.0 \cite{bib:quinlan_2020} e o \textit{Random Forest} \cite{bib:breiman_2001}, além da versão de código aberto do C4.5, o J48.

Já os modelos baseados em classificador linear não são tão simples de ser treinados, pois se espera que os dados estejam, de alguma forma, agrupados em subconjuntos. Os algoritmos dessa classe tentam gerar uma equação algébrica capaz de separar um universo de N planos em regiões, no qual cada região corresponde a uma resposta final. Sendo assim, os modelos gerados a partir deles são mais difíceis de serem implementados. Contudo, \citet{bib:livroKubat} afirma que esse conceito de aprendizado de máquina é mais adequado para ser utilizado em conjuntos de dados com atributos mutuamente independentes. Fazem parte desse conceito os algoritmos \textit{Linear Discriminant Function} (LDF) \cite{bib:shumway_1974}, \textit{Support Vector Machine} (SVM) \cite{bib:hearst_1998} e o classificador linear \textit{Naïve-Bayes} \cite{bib:lewis_1998}.

Por fim, podemos encontrar entre os trabalhos de transcodificação de vídeo o uso de modelos de aprendizagem profunda (do inglês, \textit{Deep Learning}), cujo diferencial é a exigência de um grande número de dados para treinamento, o que eleva consideravelmente o custo computacional para utilizá-lo e exige um espaço de armazenamento alto. Apesar da probabilidade de acerto desses algoritmos serem muito interessantes, seu uso torna-se difícil em ambientes de restrição energética ou em soluções implementadas em hardware. Como veremos no capítulo \ref{cap:3}, na literatura científica em transcodificação de vídeo, poderemos encontrar soluções que empregam os algoritmos \textit{Convolutional Neural Network} (CNN) \cite{bib:koushik_2016} e \textit{Long Short-Term Memory} (LSTM) \cite{bib:graves_2012}.

Diante de tantas opções de conceitos e algoritmos de aprendizado de máquina, o número de possíveis modelos treinados para solucionar um mesmo problema é exponencial, já que cada nova possibilidade é um fator multiplicador a todas as combinações já consideradas. Por consequência, é preciso saber quais deles são mais ou menos apropriados para resolver determinados problemas. Para possibilitar a averiguação da viabilidade desses modelos, temos que mensurar a confiabilidade de cada um deles, conforme veremos na próxima seção.
